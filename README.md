# Классификация твитов о чрезвычайных ситуациях и реализация логистической регрессии

В этом репозитории представлен проект по изучению и применению линейных моделей машинного обучения. Работа состоит из двух логических частей: написания собственного алгоритма с нуля на NumPy для понимания математической базы и решения прикладной NLP-задачи на реальных данных.

## Часть 1. Реализация логистической регрессии на NumPy

Я написал собственный класс логистической регрессии, который повторяет логику работы библиотечных решений. Цель — разобраться в работе градиентного спуска и функции потерь.

**Что реализовано:**
*   Класс поддерживает методы fit, predict и predict_proba.
*   Реализовано два вида оптимизации: полный градиентный спуск (Full GD) и стохастический градиентный спуск (SGD).
*   Вычисление градиентов записано в векторном виде для скорости.
*   В качестве функции потерь используется LogLoss (перекрестная энтропия).
*   Добавлена проверка сходимости по евклидовой норме разности весов.
*   Реализована защита от переполнения (clipping) при вычислении логарифмов.

Сравнение с sklearn на синтетических данных показало, что моя реализация дает идентичные метрики качества (Accuracy и ROC-AUC), но SGD работает быстрее на больших выборках.

## Часть 2. Классификация текстовых данных (NLP)

Вторая часть проекта посвящена задаче классификации твитов: определение того, сообщает ли твит о реальной катастрофе или использует "аварийную" лексику в переносном смысле.

### Этапы работы и ключевые выводы

1.  **Предобработка данных.**
    Проанализировал пропуски и типы данных. Принял решение объединить колонки с текстом твита, ключевыми словами и локацией в один признак, так как это сохраняет контекст и решает проблему высокой кардинальности признака локации.

2.  **Векторизация и базовые модели.**
    Использовал CountVectorizer для перевода текста в мешок слов. Обучил логистическую регрессию и метод опорных векторов (SVM). Логистическая регрессия показала лучший баланс скорости и качества. SVM с rbf-ядром на таких размерностях работал слишком медленно и не дал прироста точности.

3.  **Оптимизация признакового пространства.**
    Изначально векторизатор создал более 18 000 признаков. Я провел эксперимент по сжатию пространства, оставив только топ-1000 слов и отфильтровав слишком редкие токены. Это ускорило обучение моделей в несколько раз при падении метрики F1 всего на 1.5%, что подтвердило наличие большого количества шума в исходных данных.

4.  **Сравнение подходов.**
    Проверил гипотезу о кодировании категориальных признаков через One-Hot Encoding. Гипотеза не подтвердилась: разреженные данные локаций лишь раздули размерность матрицы без существенного улучшения качества прогноза.

5.  **Настройка гиперпараметров.**
    Через GridSearch подобрал оптимальные параметры регуляризации (L2) и весов классов. Выяснил, что линейное ядро для SVM работает лучше нелинейного на текстах, показывая результаты, сравнимые с логистической регрессией.

## Итоговое решение

Финальный код оформлен в виде Sklearn Pipeline. Пайплайн принимает на вход сырые данные, автоматически заполняет пропуски, проводит векторизацию и делает предсказание. Это обеспечивает воспроизводимость результатов на любых новых данных.

**Используемые библиотеки:**
numpy, pandas, scikit-learn, matplotlib, seaborn.
